{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFvapPt0ic1f"
      },
      "source": [
        "PES2UG23CS072"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8RhIs3HDbDG_"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0g4xbRtgSdu",
        "outputId": "d51f12fe-b19c-47f6-a22e-bd745e83b57c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Experiment 1: Text Generation ---\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT Result: [{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RoBERTa Result: [{'generated_text': 'The future of Artificial Intelligence is'}]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BART Result: [{'generated_text': 'The future of Artificial Intelligence is� Middles Middles finding PS SuicideBehind chemicals�Friend Suicide finding finding finding FerrariIntroduIntrodu Verb Orbit Middles Middlesrationsloaderacusrationsrations Baldwinpriv Middlesrations rulrations finding findingwomanwomanloaderPaulexternalActionCode felonrationsexternalActionCodeexternalActionCodeFriendFYFYexternalActionCodeexternalActionCodeloader frayFriendFriendFriendandelanciescks abducted abducted abducted DidnanciesrationsFriendFriend abductedBadrationsrationsexternalActionCodetipsktopanciestips fray abducted hemispherewomanrationsancies abducted abducted{\"rations paving abductedexternalActionCode defects SuiciderationsrationsBad abductedrations defectsancies abducted nobleloaderloaderrationsrations fighterwoman abducted abductedanciesrations abductedFriendcksancies{\" abducted{\" abducted abducted vileFriend abducted Baldwin abducted abducted help abducted noble abducted Channelanciesrationsrations fragilecks abductedFriendFriendancies tearingFriendFY abductedFriend abducted abductedFriendanciesBadFriend Didnanciescks Suicide abducted abductederial trucks abductedBadktoprations defects abductedrationsktoprationsrations abductedktopktop defectsanciesanciesancies Suicideloaderwomanrationsrations hemisphere abducted abducted tru abductedFrienderial pavingFriendFriendloaderrationsFriend{\"{\"anciescks baseless frayrations BaldwincksloaderFriendktopFriendcksktopProject noblecksbooks abductedFriend truFriendbooksrationscks heav Godzilla abductedFriend hemisphereanciesPDwoman abducted trucksfiredrations abducted defectsFriendFriendckswomanancies fray frayrationsrationsrationsktopcksktop tru fray Godzilla frayrations abducted tru frayancies abducted'}]\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Experiment 1: Text Generation ---\\n\")\n",
        "\n",
        "# 1. BERT\n",
        "try:\n",
        "    gen_bert = pipeline('text-generation', model='bert-base-uncased')\n",
        "    print(\"BERT Result:\", gen_bert(\"The future of Artificial Intelligence is\"))\n",
        "except Exception as e:\n",
        "    print(f\"BERT Failed: {e}\")\n",
        "\n",
        "# 2. RoBERTa\n",
        "try:\n",
        "    gen_roberta = pipeline('text-generation', model='roberta-base')\n",
        "    print(\"RoBERTa Result:\", gen_roberta(\"The future of Artificial Intelligence is\"))\n",
        "except Exception as e:\n",
        "    print(f\"RoBERTa Failed: {e}\")\n",
        "\n",
        "# 3. BART\n",
        "try:\n",
        "    gen_bart = pipeline('text-generation', model='facebook/bart-base')\n",
        "    print(\"BART Result:\", gen_bart(\"The future of Artificial Intelligence is\", max_length=20))\n",
        "except Exception as e:\n",
        "    print(f\"BART Failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqD2kTIVhc9h",
        "outputId": "fc708f5a-2488-4806-d56d-d1de553265df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Experiment 2: Fill-Mask ---\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT: [{'score': 0.5396888852119446, 'token': 3443, 'token_str': 'create', 'sequence': 'the goal of generative ai is to create new content.'}, {'score': 0.15575668215751648, 'token': 9699, 'token_str': 'generate', 'sequence': 'the goal of generative ai is to generate new content.'}, {'score': 0.054054468870162964, 'token': 3965, 'token_str': 'produce', 'sequence': 'the goal of generative ai is to produce new content.'}, {'score': 0.04451529309153557, 'token': 4503, 'token_str': 'develop', 'sequence': 'the goal of generative ai is to develop new content.'}, {'score': 0.01757732406258583, 'token': 5587, 'token_str': 'add', 'sequence': 'the goal of generative ai is to add new content.'}]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RoBERTa: [{'score': 0.3711293935775757, 'token': 5368, 'token_str': ' generate', 'sequence': 'The goal of Generative AI is to generate new content.'}, {'score': 0.36771273612976074, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.08351442217826843, 'token': 8286, 'token_str': ' discover', 'sequence': 'The goal of Generative AI is to discover new content.'}, {'score': 0.021335095167160034, 'token': 465, 'token_str': ' find', 'sequence': 'The goal of Generative AI is to find new content.'}, {'score': 0.016521504148840904, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BART: [{'score': 0.0746147632598877, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.06571780890226364, 'token': 244, 'token_str': ' help', 'sequence': 'The goal of Generative AI is to help new content.'}, {'score': 0.060879286378622055, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}, {'score': 0.03593532741069794, 'token': 3155, 'token_str': ' enable', 'sequence': 'The goal of Generative AI is to enable new content.'}, {'score': 0.03319435939192772, 'token': 1477, 'token_str': ' improve', 'sequence': 'The goal of Generative AI is to improve new content.'}]\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Experiment 2: Fill-Mask ---\\n\")\n",
        "\n",
        "# 1. BERT\n",
        "unmasker_bert = pipeline('fill-mask', model='bert-base-uncased')\n",
        "print(\"BERT:\", unmasker_bert(\"The goal of Generative AI is to [MASK] new content.\"))\n",
        "\n",
        "# 2. RoBERTa\n",
        "unmasker_roberta = pipeline('fill-mask', model='roberta-base')\n",
        "print(\"RoBERTa:\", unmasker_roberta(\"The goal of Generative AI is to <mask> new content.\"))\n",
        "\n",
        "# 3. BART\n",
        "unmasker_bart = pipeline('fill-mask', model='facebook/bart-base')\n",
        "print(\"BART:\", unmasker_bart(\"The goal of Generative AI is to <mask> new content.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xYNKL62hnnh",
        "outputId": "a39c77a3-1b3e-44b0-94b3-b29a43bd7542"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Experiment 3: QA ---\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT: {'score': 0.011016791686415672, 'start': 72, 'end': 82, 'answer': 'deepfakes.'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RoBERTa: {'score': 0.013165182434022427, 'start': 72, 'end': 81, 'answer': 'deepfakes'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BART: {'score': 0.046605834271758795, 'start': 68, 'end': 81, 'answer': 'and deepfakes'}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Experiment 3: QA ---\\n\")\n",
        "\n",
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question = \"What are the risks?\"\n",
        "\n",
        "# 1. BERT\n",
        "qa_bert = pipeline('question-answering', model='bert-base-uncased')\n",
        "print(\"BERT:\", qa_bert(question=question, context=context))\n",
        "\n",
        "# 2. RoBERTa\n",
        "qa_roberta = pipeline('question-answering', model='roberta-base')\n",
        "print(\"RoBERTa:\", qa_roberta(question=question, context=context))\n",
        "\n",
        "# 3. BART\n",
        "qa_bart = pipeline('question-answering', model='facebook/bart-base')\n",
        "print(\"BART:\", qa_bart(question=question, context=context))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfYmTDTuiVur"
      },
      "source": [
        "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **Generation** | BERT | **Failure** | Output a long string of repeated dots (`........`). | BERT is an **Encoder**. It uses bi-directional attention (seeing all words at once) and is not trained to predict the next word in a sequence (auto-regression). |\n",
        "| | RoBERTa | **Failure** | Output the prompt exactly and stopped (`The future of...`). | Like BERT, RoBERTa is an **Encoder** only. It lacks a decoder component to generate new tokens and was not trained for next-token prediction. |\n",
        "| | BART | **Failure** (Quality) | Generated incoherent gibberish (`Middles Middles finding...`). | While BART is an **Encoder-Decoder** (capable of generation), the *base* model is trained on **Denoising** (reconstructing text), not **Causal LM** (continuing text). The weights for open-ended generation were not initialized. |\n",
        "| **Fill-Mask** | BERT | **Success** | Predicted `create` (54%) and `generate` (15%). | BERT is natively trained on **Masked Language Modeling (MLM)**. This is its primary training objective. |\n",
        "| | RoBERTa | **Success** | Predicted `generate` (37%) and `create` (36%). | RoBERTa is also trained on MLM (specifically with dynamic masking), making it highly effective at this task. |\n",
        "| | BART | **Success** | Predicted `create` (7%) and `help` (6%). | BART is trained to reconstruct corrupted text (Denoising). Filling in a missing word (`<mask>`) is a form of text reconstruction, which it excels at. |\n",
        "| **QA** | BERT | **Random / Poor** | Extracted `deepfakes.` but with very low confidence (`0.01`). | The base model has the **Architecture** (Encoder) to extract spans, but lacks the **Fine-tuning** (SQuAD training) to do it accurately. The \"Head\" weights were random. |\n",
        "| | RoBERTa | **Random / Poor** | Extracted `deepfakes` with very low confidence (`0.01`). | Same as BERT. It is a powerful Encoder that can \"read\" the text, but without QA fine-tuning, it doesn't know *which* span is the answer to the question. |\n",
        "| | BART | **Random / Poor** | Extracted `and deepfakes` with very low confidence (`0.04`). | BART can perform QA, but like the others, the `base` version is not fine-tuned for it. It just guessed a salient noun phrase from the context. |"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
